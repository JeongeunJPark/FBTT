{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 왜 손실함수를 설정하는가?       \n",
    "\n",
    "- 신경망 학습에서는 최적의 매개변수(W, B)를 탐색할 때 손실 함수 값을 가장 작게 하는 값을 찾는다.       \n",
    "- 이 때 매개변수의 미분 값을 계산하고, 미분 값을 단서로 W, B값을 서서히 갱신하는 과정을 반복한다.      \n",
    "- 정확도를 지표로 하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문에 방향을 잃는다.    \n",
    "---\n",
    "- 매개변수를 조정하면 정확도는 아예 움직이지 않거나, 계단뛰기 하듯이 변화한다.   \n",
    "- 반면 손실함수는 비교적 연속된 값을 보이며 변화한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 수치 미분      \n",
    "- 경사법에서는 기울기 값을 기준으로 나아갈 방향을 정하지!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 미분    \n",
    "\n",
    "- __미분 : 특정 순간의 변화량__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미분 구현의 나쁜 예\n",
    "def numerical_diff(f, x):\n",
    "    h = 10e-50\n",
    "    return (f(x+h)-f(x))/h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서, 가장 작은 값을 얻기 위해 하드코딩한 h값은 '반올림 오차' 를 일으킬 수 있음    \n",
    "- 따라서 경험적으로 h값은 10$^-4$ 값이 적절함         \n",
    "- 또한, 미분은 '기울기' 값이지만 위의 계산으로는 '차분' 을 구할 수 밖에 없으므로 엄밀히 일치하지는 못한다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__이를 개선하기 위해 중앙 차분(중심 차분) 방법을 쓰기도 한다__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이게 중앙차분\n",
    "def numerical_diff(f, x):\n",
    "    h=1e-4\n",
    "    return (f(x+h)-f(x-h))/(2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이런 경우들처럼, __아주 작은 차분으로 미분하는 것 == 수치미분__      \n",
    "- 수식을 전개한 것은 해석적 미분. 해석적 미분이 바로 '오차를 포함하지 않는 진짜 미분   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 수치 미분의 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(x) = 0.01x^2 + 0.1x\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAiWUlEQVR4nO3deXhV1b3/8feXhBAIcwbmAGGSQcZAglKqOFzlUlGrFixSlUGtVu291uut/Vlbe68d1OvUWlFQkNEJBxxxlgqBAGEM8xSmDIwJgYQk6/dHwr2YJiFAdvY5J5/X8+Th5Ox9sr6uc/JxZ++11zLnHCIiEnrq+V2AiIh4QwEvIhKiFPAiIiFKAS8iEqIU8CIiISrc7wJOFxMT4zp16uR3GSIiQWP58uU5zrnYirYFVMB36tSJ1NRUv8sQEQkaZrazsm06RSMiEqIU8CIiIUoBLyISojwNeDNrbmZvmtkGM0s3s6FeticiIv/H64uszwAfO+duMLMIoJHH7YmISBnPAt7MmgLDgVsBnHOFQKFX7YmIyPd5eYomAcgGXjGzlWb2splFedieiIicxsuADwcGAi845wYAx4CHyu9kZpPNLNXMUrOzsz0sR0Qk8CzfeZCXvtnmyc/2MuB3A7udcyll379JaeB/j3NuinMu0TmXGBtb4c1YIiIhKX3fUW57ZRmzUnZyrKCoxn++ZwHvnNsPZJhZj7KnLgPWe9WeiEgw2ZFzjFumLqVRRDivTUgiqkHNXxL1ehTNL4BZZSNotgG3edyeiEjA23/kBOOmplBcUsLcyUPp0NKbAYaeBrxzLg1I9LINEZFgcji/kPHTUjh0rJA5k5PpGtfEs7YCarIxEZFQdqygiFtfWcaOA/m8ettg+rZv7ml7mqpARKQWnDhZzMTpqazZc4Tnxw7goi4xnrepgBcR8VhhUQk/n7WCJdsP8OSN/biyd+taaVcBLyLioeISxy/npfHFhiz+69oLuXZAu1prWwEvIuKRkhLHf7y1mg/W7OPhkT25OSm+VttXwIuIeMA5x+/eX8eby3dz32XdmDQ8odZrUMCLiHjgL59sZPrinUwc1pn7L+/mSw0KeBGRGvbXL7fwt6+2MnZIPA//a0/MzJc6FPAiIjXo1X9s5y+fbGR0/7b84do+voU7KOBFRGrM66kZPPr+eq7o1YonbuxHWD3/wh0U8CIiNWLB6r089NZqftAthudvHkD9MP/j1f8KRESC3BcbMrl/bhqDOrbgxVsG0SA8zO+SAAW8iMh5+XZzNnfOXEHPNk2ZeutgGkUEzhRfCngRkXP03dYcJk5PJSEmihm3D6FpZH2/S/oeBbyIyDlYuv0gE15NJb5lI2ZNTKJFVITfJf0TBbyIyFlavvMQt72ylDbNI5k1KYnoxg38LqlCCngRkbOwKuMwt05bSmyTBsyZlExck0i/S6qUAl5EpJrW7jnCLVNTaB5Vn9mTkmnVNHDDHRTwIiLVkr7vKOOmptAksj6zJybTtnlDv0s6IwW8iMgZbM7MZdzLKUSGhzF7UpJni2TXNAW8iEgVtmbnMfalFOrVM2ZPSqJjdJTfJVWbAl5EpBI7co5x80tLAMecSUkkxDb2u6SzooAXEalAxsF8bn5pCYVFJcyamEzXuCZ+l3TWAueeWhGRAJFxMJ8xU5ZwrLCY2ZOS6NE6+MIdFPAiIt+z60A+Y6Ys5lhhMbMmJtG7bTO/Szpnnga8me0AcoFioMg5l+hleyIi52PngWOMnbKE/JOl4d6nXfCGO9TOEfylzrmcWmhHROSc7cg5xtiXlnDiZDGzJybTq21Tv0s6bzpFIyJ13vac0iP3wuISZk9Kpmeb4A938H4UjQM+NbPlZja5oh3MbLKZpZpZanZ2tsfliIh837bsPMZMWVwW7kkhE+7gfcBf7JwbCFwN3G1mw8vv4Jyb4pxLdM4lxsbGelyOiMj/2Zqdx5gpSygqdsyZlMwFrUMn3MHjgHfO7S37NwuYDwzxsj0RkeraklUa7iXOMWdyctAOhayKZwFvZlFm1uTUY+BKYK1X7YmIVNeWrFzGTFmCczBnUjLdW4VeuIO3F1lbAfPN7FQ7s51zH3vYnojIGW3OzGXsS0swM+ZMSqZrXHBNP3A2PAt459w2oJ9XP19E5Gxt3J/LT1+uG+EOmotGROqItXuO8JMpiwmrZ8ydHPrhDgp4EakDlu88xNiXlhAVEc7rdwylS5DNCnmudKOTiIS0xVsPMGH6MuKaNGDWpGTaBcFKTDVFAS8iIevrTdlMnpFKfMtGzJqYRFyAr6Fa0xTwIhKSFq7P5O5ZK+gS15iZE4YQ3biB3yXVOgW8iIScBav3cv/cNHq3a8aM24bQrFF9v0vyhS6yikhIeWv5bu6ds5IB8c2ZOaHuhjvoCF5EQsislJ08PH8tF3eN5qXxiTSKqNsRV7f/60UkZExdtJ3HFqxnxAVx/O2nA4msH+Z3Sb5TwItI0Pvrl1v4yycbubpPa54ZM4CIcJ19BgW8iAQx5xx//HgDL369jWv7t+WJG/sRHqZwP0UBLyJBqbjE8Zt31jBnaQbjkuP5/TV9qFfP/C4roCjgRSToFBaV8MvX0/hg9T7uvrQLD1zZg7KZa+U0CngRCSrHC4u5c+Zyvt6Uza9HXsDk4V38LilgKeBFJGgcOX6SCa8uY8WuQ/zpxxfyk8HxfpcU0BTwIhIUsnMLGD9tKVuycnn+5oGMvLCN3yUFPAW8iAS83YfyGfdyCplHC5j6s8EM7x7rd0lBQQEvIgFtS1Yu415eSn5hETMnJjGoYwu/SwoaCngRCVirdx/mZ9OWElavHvPuGErPNk39LimoKOBFJCAt2XaAidNTad6oPjMnJNEpJsrvkoKOAl5EAs5Ha/Zx37w0OrZsxGsTkmjdrG4t1FFTFPAiElBeW7KTR95dy4AOzZl262CaN4rwu6SgpYAXkYDgnOOphZt47ostXN4zjufGDqRhhGaEPB8KeBHxXVFxCb95Zy1zl2Xwk8QO/Nd1fTRpWA3wPODNLAxIBfY450Z53Z6IBJfjhcX8Ys5KPkvP5BcjuvJvV3TXvDI1pDaO4O8D0gGNbxKR7zmcX8iE6ams2HWIx0b35pahnfwuKaR4+jeQmbUH/hV42ct2RCT47D18nBv+vpg1u4/wt5sHKtw94PUR/NPAg0CTynYws8nAZID4eE0cJFIXbMrMZfzUpRwrKGLGhCEkJ0T7XVJI8uwI3sxGAVnOueVV7eecm+KcS3TOJcbGan4JkVC3bMdBbnjhO0qc4/U7hyrcPeTlEfzFwDVmNhKIBJqa2Uzn3DgP2xSRAPbx2v3cN3cl7Vo0ZMbtQ2jfopHfJYU0z47gnXP/6Zxr75zrBIwBvlC4i9RdUxdt565Zy+nVtilv3nmRwr0WaBy8iHiquMTx2IL1vPrdDq7q3Zqnx/Qnsr5uYKoNtRLwzrmvgK9qoy0RCRzHC4u5d+5KFq7PZMKwzvx6ZE/CtDB2rdERvIh4Iju3gInTl7F6zxEe/VEvbr24s98l1TkKeBGpcVuz87j1laVk5xbw4rhBXNm7td8l1UkKeBGpUUu3H2TSjFTqhxlzJw+lf4fmfpdUZyngRaTGvLdqLw+8vor2LRvy6q1DiI/WSBk/KeBF5Lw553jh6638+eONDOnckim3DNI87gFAAS8i5+VkcQmPvLuOOUt3cU2/tvzlxr40CNcwyECggBeRc3Yk/yR3z17Boi053HVJF351ZQ/qaRhkwFDAi8g52ZFzjNunLyPjYD5/vqEvNyV28LskKUcBLyJnbfHWA9w1q3QewZkTkkjShGEBSQEvImdl3rJdPDx/LR2jGzHt1sF0jI7yuySphAJeRKqluMTxp483MOWbbfygWwzP3zyQZg3r+12WVEEBLyJnlFdQxP1zV/JZehbjh3bkkVG9tCh2EFDAi0iV9hw+zoRXl7E5K4/fj+7NeC2tFzQU8CJSqRW7DjF5xnIKThbzyq2DGd5dq64FEwW8iFTo3bQ9/OrN1bRuGsmcSUl0a1Xp0soSoBTwIvI9xSWOv3yykb9/vZUhnVry91sG0TJK0w4EIwW8iPyvI8dPct/clXy1MZubk+J59Ee9iQjXxdRgpYAXEQC2ZOUxaUYqGQfz+cO1fRiX3NHvkuQ8KeBFhM/TM7l/bhoR4fWYPSmZIZ1b+l2S1AAFvEgd5pzjb19t5YlPN9K7bVNevCWRds0b+l2W1BAFvEgdlV9YxK/eWM0Ha/Yxun9b/nh9XxpGaJrfUKKAF6mDMg7mM2lGKpsyc/n1yAuY9IMEzDTNb6hRwIvUMd9tzeHuWSsoLnG8ctsQfqibl0JWtQLezOKAi4G2wHFgLZDqnCvxsDYRqUHOOV75xw7+68N0OsdE8dL4RDrHaCbIUFZlwJvZpcBDQEtgJZAFRALXAl3M7E3gSefc0QpeGwl8AzQoa+dN59xva7R6EamWYwVFPPT2Gt5ftZcrerXiqZv60SRSM0GGujMdwY8EJjnndpXfYGbhwCjgCuCtCl5bAIxwzuWZWX1gkZl95Jxbcr5Fi0j1bc3O487XlrM1O48Hr+rBncO7aFm9OqLKgHfO/aqKbUXAO1Vsd0Be2bf1y77c2ZcoIufq47X7eeCNVUSE1+O1CUlc3DXG75KkFlXrHmQze83Mmp32fScz+7warwszszRKT+0sdM6lVLDPZDNLNbPU7OzssyhdRCpTVFzC4x+lc+fM5XSJa8yCXwxTuNdB1Z1kYhGQYmYjzWwS8Cnw9Jle5Jwrds71B9oDQ8ysTwX7THHOJTrnEmNjdTVf5Hzl5BVwy9SlvPj1NsYlx/P6Hcm01c1LdVK1RtE45140s3XAl0AOMMA5t7+6jTjnDpvZV8BVlI7AEREPrNh1iJ/PXMGh/EKeuLEfNwxq73dJ4qPqnqK5BZgGjAdeBT40s35neE2smTUve9wQuBzYcD7FikjFnHPMWLyDn7y4mPrhxts/v0jhLtW+0enHwDDnXBYwx8zmUxr0A6p4TRtgupmFUfo/ktedcwvOp1gR+Wf5hUX8Zv5a3l65hxEXxPE/N/WnWSMNgZTqn6K5ttz3S80s6QyvWU3V/wMQkfO0OTOXn89awZbsPP7tiu7cc2lXDYGU/1XlKRoz+42ZVThvqHOu0MxGmNkob0oTkaq8tXw31zz/Dw7lF/La7Unce1k3hbt8z5mO4NcA75vZCWAFkE3pnazdgP7AZ8B/e1mgiHzf8cJiHnl3LW8s301yQkueHTOAuKaRfpclAehMAX+Dc+5iM3uQ0rHsbYCjwExgsnPuuNcFisj/2ZJVekpmc1Ye947oyn2XdydMR+1SiTMF/CAz6wj8FLi03LaGlE48JiK14O0Vu3l4/loaRYQx4/Yh/KCb7huRqp0p4P8OfAwkAKmnPW+UTjuQ4FFdIlLmeGExj763jnmpGSR1bsmzYwfQSqdkpBrONBfNs8CzZvaCc+6uWqpJRMpsycrl7lkr2ZSVyy9GdOW+y7oRHlbdG9ClrqvuMEmFu0gtcs4xb1kGj76/jqiIcKbfNoThWphDzpJWdBIJMEeOn+TXb6/hgzX7GNY1hqdu6qdRMnJOFPAiASR1x0Hum5tG5tETPHT1BUz+QYLGtss5U8CLBIDiEsdfv9zC059tokPLRrx510X079Dc77IkyCngRXy29/Bx7p+XxtLtB7luQDt+P7q3ltOTGqGAF/HRx2v38x9vraaouISnburH9QM1A6TUHAW8iA/yC4v4wwfpzE7ZxYXtmvHs2AF0jonyuywJMQp4kVqWlnGYX85LY8eBY9wxPIF/v7IHEeEa2y41TwEvUkuKikt4/sstPPfFFlo3jWTOpGSSE6L9LktCmAJepBZszznG/fPSWJVxmOsGtON3o3vTVBdSxWMKeBEPOeeYszSDxxasJyK8Hs/fPIBRfdv6XZbUEQp4EY9k5xbw0Fur+XxDFsO6xvDEjf1o3Ux3pErtUcCLeGDh+kweems1uQVFPDKqF7de1El3pEqtU8CL1KAj+Sf53YJ1vL1iDz3bNGXOmP50b9XE77KkjlLAi9SQLzdm8dBbq8nJK+TeEV25Z0Q3DX8UXyngRc5T7omT/GFBOvNSM+gW15iXxifSt31zv8sSUcCLnI9Fm3N48M1V7D96gjt/2IX7L+9GZP0wv8sSARTwIufkWEERj3+Uzswlu0iIjeLNuy5iYHwLv8sS+R7PAt7MOgAzgNZACTDFOfeMV+2J1JYl2w7wqzdXsfvQcSYO68wD/9JDR+0SkLw8gi8C/t05t8LMmgDLzWyhc269h22KeCb3xEn++NEGZqXsomN0I16/YyiDO7X0uyyRSnkW8M65fcC+sse5ZpYOtAMU8BJ0Pk/P5DfvrCXz6AkmDuvMv13ZnUYROsMpga1WPqFm1gkYAKRUsG0yMBkgPj6+NsoRqbYDeQX87v31vLdqLz1aNeGFcYO00pIEDc8D3swaA28B9zvnjpbf7pybAkwBSExMdF7XI1IdzjneTdvL795fR15BEb+8vDt3XdJF49olqHga8GZWn9Jwn+Wce9vLtkRqyt7Dx3l4/hq+3JjNgPjm/OnHfXU3qgQlL0fRGDAVSHfOPeVVOyI1paTEMStlJ3/8aAMlDh4Z1YufXdSJMM0hI0HKyyP4i4FbgDVmllb23K+dcx962KbIOUnfd5Rfz1/Dyl2HGdY1hsevv5AOLRv5XZbIefFyFM0iQIc+EtDyC4t4+rPNTF20neYN6/PUTf24bkA7Sv8AFQluGuclddZn6zP57Xvr2HP4OGMGd+Chqy+geaMIv8sSqTEKeKlz9h05zqPvreOTdZl0b9WYN+7UDUsSmhTwUmcUFZcwffFOnvp0I8XO8eBVPZg4LEFDHyVkKeClTli56xD/7921rN1zlEt6xPLY6D66iCohTwEvIe1AXgF/+ngDr6fuJq5JA/5680BGXthaF1GlTlDAS0gqKi5hVsounvx0I/mFxdwxPIFfXNaNxg30kZe6Q592CTnLdhzkkXfXkb7vKMO6xvDoNb3pGtfY77JEap0CXkJG1tETPP7RBuav3EPbZpG88NOBXNVHp2Ok7lLAS9A7WVzC9O928PRnmyksKuGeS7vy80u7aDpfqfP0GyBByznHlxuz+MMH6WzLPsYlPWL57Y960zkmyu/SRAKCAl6C0qbMXB5bsJ5vN+eQEBPFy+MTuaxnnE7HiJxGAS9B5eCxQv5n4SZmL91FVEQY/29UL25J7qiblUQqoICXoFBYVMKMxTt45vPN5BcWMy4pnvsv706LKM0dI1IZBbwENOccC9dn8t8fprPjQD6X9Ijl4ZE96aYFOETOSAEvAWtVxmEe/yidJdsO0jWuMa/cNphLe8T5XZZI0FDAS8DZeeAYf/5kIx+s3kd0VAS/H92bsUPiqR+m8+wiZ0MBLwEjJ6+A5z7fzKyUXdQPq8e9I7oyaXgCTSLr+12aSFBSwIvv8guLePnb7Uz5ZhvHTxbzk8EduP+ybsQ1jfS7NJGgpoAX3xQVlzAvNYOnP9tMdm4B/9K7FQ9edQFdYjVvjEhNUMBLrSspcXywZh//89kmtmUfI7FjC/4+biCDOmpVJZGapICXWnNqyONTCzexYX8u3Vs1Zsotg7iiVyvdgSriAQW8eM45x7ebc3jy042s2n2EzjFRPDOmP6P6tiWsnoJdxCsKePFUyrYDPPnpJpbuOEi75g358w19uX5AO8I15FHEcwp48URaxmGe/HQj327OIa5JAx4b3ZubBnegQXiY36WJ1BkKeKlRy3ce4rkvNvPVxmxaRkXw8MiejEvuSMMIBbtIbfMs4M1sGjAKyHLO9fGqHQkMKdsO8NwXW1i0JYeWURE8eFUPxg/tpDVQRXzk5W/fq8DzwAwP2xAfOedYvPUAz3y+mZTtB4lp3ICHR/bkp8nxWk1JJAB49lvonPvGzDp59fPFP6dGxTz7+WZSdx6iVdMG/PZHvRg7JJ7I+joVIxIofD/MMrPJwGSA+Ph4n6uRqpSUOBamZ/LCV1tJyzhM22aRPDa6NzcmdlCwiwQg3wPeOTcFmAKQmJjofC5HKlBQVMw7K/fw4jfb2JZ9jA4tG/L49Rfy44HttZKSSADzPeAlcOWeOMnslF1M+8d2Mo8W0LttU54bO4Cr+7TWOHaRIKCAl3+SlXuCV/6xg5lLdpJ7ooiLu0bzxI39GNY1RlMKiAQRL4dJzgEuAWLMbDfwW+fcVK/ak/O3NTuPl7/dzlsrdnOyuISRfdpwxw8T6Nu+ud+licg58HIUzVivfrbUHOcci7bkMG3Rdr7cmE1EeD1+PLA9k4cn0Dkmyu/yROQ86BRNHXXiZOmF02n/2M6mzDxiGjfgl5d35+akeGKbNPC7PBGpAQr4Oibr6AleW7KTWSm7OHiskF5tmvLEjf34Ub82midGJMQo4OuIVRmHefW7HSxYvZeiEscVPVtx+7DOJHVuqQunIiFKAR/CjhcW8/6qvcxM2cnq3UeIighjXHJHbr2oEx2jdX5dJNQp4EPQtuw8ZqXs4o3UDI6eKKJ7q8Y8Nro31w5oR5PI+n6XJyK1RAEfIoqKS/gsPZOZS3axaEsO9cOMq/q0YVxSPEN0GkakTlLAB7ndh/J5I3U385ZlsP/oCdo2i+SBK7tz0+AOxDWJ9Ls8EfGRAj4IFRQV8+m6TF5PzWDRlhwAhnWN4fejezPigjhNIyAigAI+qKTvO8q8ZRm8k7aHw/knade8IfeO6MaNie1p36KR3+WJSIBRwAe4oydO8l7aXl5PzWD17iNEhNXjit6t+EliBy7uGkNYPZ1bF5GKKeADUGFRCd9symZ+2h4+W59JQVEJF7RuwiOjenHdgHa0iIrwu0QRCQIK+ADhnGNlxmHeWbmH91ft5VD+SVpGRTBmcAeuH9ievu2baSSMiJwVBbzPtucc452Ve3gnbQ87D+TTILweV/RqxXUD2jG8eyz1dcFURM6RAt4Hew8f58M1+1iweh9pGYcxg6EJ0dxzaVeu6tNaNyOJSI1QwNeSfUeO8+Ga/Xywei8rdh0GoFebpvzn1RdwTf+2tGnW0N8CRSTkKOA9tP/ICT5cs48P1uxj+c5DQGmo/+pfejDywjaab11EPKWAr2E7co6xcH0mn6zbT2pZqPds05QHruzOyAvbkBDb2OcKRaSuUMCfp5ISR9ruwyxcn8ln6zPZnJUHlIb6v1/RnZF929BFoS4iPlDAn4MTJ4v5bmtOaainZ5GdW0BYPSOpc0tuTorn8p6t6NBSd5aKiL8U8NWUcTCfrzdl89XGbL7bmkN+YTFREWFc0iOOK3q14tIecTRrpNEvIhI4FPCVOHGymJTtB/l6YzZfbcpiW/YxANq3aMj1A9txec9WDO0SrWXuRCRgKeDLOOfYmp3Ht5tz+GpjNku2HaCgqISI8HokJ0QzLqkjP+wRS0JMlO4oFZGgUGcD3jnHroP5LN56gO+2HmDxtgNk5xYAkBATxdgh8VzSI5akztE0jNBRuogEnzoV8PuOHOe7LaVhvnjrAfYcPg5AbJMGDE2I5qIu0VzUJYb4aF0gFZHg52nAm9lVwDNAGPCyc+6PXrZ3upISx+asPFJ3HmT5jkOk7jzEroP5ALRoVJ/khGju/GECQ7tE0yW2sU67iEjI8SzgzSwM+CtwBbAbWGZm7znn1nvR3vHCYtIyDrN850FSdx5ixc5DHD1RBEBM4wgGdWzB+KEduahLDBe0bkI9zaMuIiHOyyP4IcAW59w2ADObC4wGajTgC4qKuenFJazbc4SiEgdAt7jG/GvfNgzq2JLEji3oGN1IR+giUud4GfDtgIzTvt8NJJXfycwmA5MB4uPjz7qRBuFhdI5uxMVdokns1IKB8S1o3kgLYoiIeBnwFR0yu396wrkpwBSAxMTEf9peHU+PGXAuLxMRCWleriaxG+hw2vftgb0eticiIqfxMuCXAd3MrLOZRQBjgPc8bE9ERE7j2Ska51yRmd0DfELpMMlpzrl1XrUnIiLf5+k4eOfch8CHXrYhIiIV04rOIiIhSgEvIhKiFPAiIiFKAS8iEqLMuXO6t8gTZpYN7DzHl8cAOTVYTk1RXWcvUGtTXWdHdZ29c6mto3MutqINARXw58PMUp1ziX7XUZ7qOnuBWpvqOjuq6+zVdG06RSMiEqIU8CIiISqUAn6K3wVUQnWdvUCtTXWdHdV19mq0tpA5By8iIt8XSkfwIiJyGgW8iEiICqqAN7OrzGyjmW0xs4cq2G5m9mzZ9tVmNrCW6upgZl+aWbqZrTOz+yrY5xIzO2JmaWVfj9RSbTvMbE1Zm6kVbK/1PjOzHqf1Q5qZHTWz+8vtU2v9ZWbTzCzLzNae9lxLM1toZpvL/m1RyWur/Ex6UNdfzGxD2Xs138yaV/LaKt93D+p61Mz2nPZ+jazktbXdX/NOq2mHmaVV8lov+6vCfKiVz5hzLii+KJ1yeCuQAEQAq4Be5fYZCXxE6WpSyUBKLdXWBhhY9rgJsKmC2i4BFvjQbzuAmCq2+9Jn5d7X/ZTerOFLfwHDgYHA2tOe+zPwUNnjh4A/VVJ7lZ9JD+q6Eggve/yniuqqzvvuQV2PAg9U472u1f4qt/1J4BEf+qvCfKiNz1gwHcH/7yLezrlC4NQi3qcbDcxwpZYAzc2sjdeFOef2OedWlD3OBdIpXZM2GPjSZ6e5DNjqnDvXO5jPm3PuG+BguadHA9PLHk8Hrq3gpdX5TNZoXc65T51zRWXfLqF0pbRaVUl/VUet99cpZmbATcCcmmqvuqrIB88/Y8EU8BUt4l0+RKuzj6fMrBMwAEipYPNQM1tlZh+ZWe9aKskBn5rZcitd4Lw8v/tsDJX/0vnRX6e0cs7tg9JfUCCugn387rvbKf3rqyJnet+9cE/ZqaNplZxu8LO/fgBkOuc2V7K9VvqrXD54/hkLpoCvziLe1Vro2ytm1hh4C7jfOXe03OYVlJ6G6Ac8B7xTS2Vd7JwbCFwN3G1mw8tt963PrHQpx2uANyrY7Fd/nQ0/++5hoAiYVckuZ3rfa9oLQBegP7CP0tMh5fn5+zmWqo/ePe+vM+RDpS+r4Llq91kwBXx1FvH2baFvM6tP6Zs3yzn3dvntzrmjzrm8sscfAvXNLMbrupxze8v+zQLmU/on3+n8XBz9amCFcy6z/Aa/+us0madOVZX9m1XBPr70nZn9DBgF/NSVnagtrxrve41yzmU654qdcyXAS5W051d/hQPXA/Mq28fr/qokHzz/jAVTwFdnEe/3gPFlI0OSgSOn/gTyUtn5valAunPuqUr2aV22H2Y2hNK+P+BxXVFm1uTUY0ov0K0tt5svfVam0qMqP/qrnPeAn5U9/hnwbgX71PrC8mZ2FfAfwDXOufxK9qnO+17TdZ1+3ea6Stqr9f4qczmwwTm3u6KNXvdXFfng/WfMi6vGXn1ROuJjE6VXlR8ue+5O4M6yxwb8tWz7GiCxluoaRumfTauBtLKvkeVquwdYR+lV8CXARbVQV0JZe6vK2g6kPmtEaWA3O+05X/qL0v/J7ANOUnrENAGIBj4HNpf927Js37bAh1V9Jj2uawul52RPfc7+Xr6uyt53j+t6rezzs5rSAGoTCP1V9vyrpz5Xp+1bm/1VWT54/hnTVAUiIiEqmE7RiIjIWVDAi4iEKAW8iEiIUsCLiIQoBbyISIhSwIuIhCgFvIhIiFLAi1TCzAaXTZ4VWXa34zoz6+N3XSLVpRudRKpgZn8AIoGGwG7n3OM+lyRSbQp4kSqUzf+xDDhB6XQJxT6XJFJtOkUjUrWWQGNKV+KJ9LkWkbOiI3iRKpjZe5SuotOZ0gm07vG5JJFqC/e7AJFAZWbjgSLn3GwzCwO+M7MRzrkv/K5NpDp0BC8iEqJ0Dl5EJEQp4EVEQpQCXkQkRCngRURClAJeRCREKeBFREKUAl5EJET9fy3Z/7BKPv6hAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x=5 일 때와 x=10일 때의 미분값을 찾아보자.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2999999999986347"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 편미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f(x0, x1) = x0^2 + x1^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어느 변수에 대해 미분해야 하지? (x0, x1 두개인데!)       \n",
    "- __편미분 : 변수가 여럿인 함수에 대한 미분___     \n",
    "- 목표 변수에 초점을 맞추고, 나머지는 고정하여 구한다.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 기울기        \n",
    "\n",
    "* 기울기 : 모든 변수의 편미분을 벡터로 정리한 것 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 #0.0001\n",
    "    \n",
    "    #x와 형상이 같고 원소가 모두 0인 배열을 만든다.\n",
    "    \n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for index in range(x.size):\n",
    "        tmp_val = x[index]\n",
    "        \n",
    "        x[index] = tmp_val+h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[index] = tmp_val-h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[index] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        #값 초기화\n",
    "        x[index] = tmp_val\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세 점( (3,4), (0,2), (3,0) )에서의 기울기를 구해 보자.\n",
    "\n",
    "numerical_gradient(function_2, np.array([3.0],[4.0]))\n",
    "\n",
    "numerical_gradient(function_2, np.array([0],[2.0]))\n",
    "\n",
    "numerical_gradient(function_2, np.array([3.0],[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __기울기가 가리키는 쪽은 각 장소에서 함수의 출력값을 가장 크게 줄이는 방향__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 경사법(경사 하강법)      \n",
    "\n",
    "- 기울기를 이용해 (손실)함수의 최소값을 찾으려는 것     \n",
    "- 함수가 극소값, 극대값, 안장점이 되는 장소에서도 기울기가 0이므로 조심!\n",
    "---\n",
    "- 경사법은 현재 위치에서 기울어진 방향으로 일정 거리만큼 이동    \n",
    "- 다시 그 움직인 자리에서 기울기 구하기     \n",
    "- 이동->기울기->이동->기울기 ......\n",
    "- 이것이 바로 경사법\n",
    "---\n",
    "- 경사 상승법 : 최대값을 찾을 때      \n",
    "- 경사 하강법 : 최소값을 찾을 때      \n",
    "- 단, 손실 함수의 부호를 반전시키면 최소값찾는것==최대값찾는것이라 중요하지 않음.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-에타 : $eta$ 학습률(한 번에 얼마나 학습할 지, 즉 w값을 얼마나 갱신할 지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr*grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0,4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.58983747e+13, -1.29524862e+12])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습률이 너무 크면\n",
    "init_x = np.array([-3.0,4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=10, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습률이 너무 작으면\n",
    "init_x = np.array([-3.0,4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- 여기서, 학습률과 같이 변경시키는 매개변수를 __하이퍼 파라미터__ 라고 함\n",
    "- 사람이 직접 설정하며, 시험을 통해 가장 좋은 값을 찾아내야 함    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 신경망에서의 기울기     \n",
    "- 가중치 매개변수에 대한 손실함수의 기울기를 구해야 함     \n",
    "- 신경망의 기울기를 구한 후에는, 가중치 매개변수를 경신하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 학습 알고리즘 구현하기    \n",
    "---\n",
    "* 전체 학습 과정\n",
    "1. 전제    \n",
    "- 신경망에는 W와 B가 있고, 이 가중치와 편향을 데이터에 적용하는 과정을 '학습' 이라고 함\n",
    "2. 미니배치 : 전체 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 '미니배치'라고 하고, 미니배치의 손실함수 값을 줄이는 것이 목표이다.     \n",
    "3. 기울기 산출 : 미니배치의 손실함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다.    \n",
    "4. 매개변수 갱신 : 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_siz, weight_init_std = 0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std *\\np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std *\\np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1)+b1\n",
    "        z1 = sigmoid(a1)\n",
    "        \n",
    "        a2 = np.dot(z1, W2) +b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy(y, t)\n",
    "    \n",
    "    # x:입력 데이터, t:정답 레이블\n",
    "    def accuracy(self, x, t):\n",
    "        y=self.predict(x)\n",
    "        y=np.argmax(y, axis=1)\n",
    "        t=np.argmax(t, aixs=1)\n",
    "        \n",
    "        accuracy = np.sum(y==t)/float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- numerical_gradient를 고속으로 수행하는 기법 == 오차역전파법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list=[]\n",
    "\n",
    "#iteration number\n",
    "iters_num = 10000\n",
    "train_size=x_train_shape[0]\n",
    "batch_siez=100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 에포크 : 1에폭 : 학습에서 훈련 데이터를 모두 소진했을 때의 횟수      \n",
    "- 훈련데이터 10,000개를 100개의 미니배치로 학습한 경우, \n",
    "- 각 100개의 미니배치는 100번 반복하면 데이터를 모두 소진하게 된다. 100번 = 1에포크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6 정리 \n",
    "\n",
    "1. 기계학습에서 사용하는 데이터셋은 훈련과 시험 데이터로 나눠 평가한다.    \n",
    "2. 훈련 데이터로 학습한 모델의 범용 능력을 시험 데이터로 평가한다.    \n",
    "3. 신경망 학습은 손실 함수를 지표로 하여, 손실 함수의 값이 작아지는 방향으로 가중치 매개변수를 갱신한다.     \n",
    "4. 가중치 매개변수를 갱신할 때는 가중치 매개변수의 기울기를 이용하고, 기울어진 방향으로 가중치의 값을 갱신하는 작업을 반복한다.    \n",
    "5. 아주 작은 값을 주었을 때의 차분으로 미분하는 것을 수치 미분이라고 한다.    \n",
    "6. 수치 미분을 이용해 가중치 매개변수의 기울기를 구할 수 있다.    \n",
    "7. 수치 미분으로 가중치 매개변수의 기울기를 구하는 것은 시간이 걸린다.     \n",
    "8. 이 시간을 줄이는 방법이 바로 오차역전파법이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
